{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 DNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-08-1 Perceptron\n",
    "- 퍼셉트론(Perceptron)\n",
    "- 선형분류기(Linear Classifier)\n",
    "- AND, OR, XOR 게이트"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron\n",
    "- 인공 신경망은, 인간의 뉴런을 본따 만든 신경망  \n",
    "- 입력 신호들의 합이 임계값을 넘으면 신호를 출력하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7055874466896057\n",
      "1000 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "10000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# XOR\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "## nn Layers\n",
    "linear = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear, sigmoid).to(device)\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%1000==0:\n",
    "        print(step, cost.item())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss가 줄어들지 않음 -> 학습이 제대로 진행되지 않음 (XOR은 퍼셉트론으로 해결 불가능)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-08-2 Multi Layer Perceptron\n",
    "- 다중 퍼셉트론(Multi Layer Perceptron)\n",
    "- 오차역전파(Backpropagation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 레이어를 학습 시킬 수 있는 방법 -> 오차역전파 (loss 값의 그라디언트를 최소화하는 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn Layers\n",
    "w1 = torch.Tensor(2, 2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2, 1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.34671515226364136\n",
      "1000 0.3467007279396057\n",
      "2000 0.3466891646385193\n",
      "3000 0.34667932987213135\n",
      "4000 0.3466709852218628\n",
      "5000 0.3466639220714569\n",
      "6000 0.346657931804657\n",
      "7000 0.34665271639823914\n",
      "8000 0.34664785861968994\n",
      "9000 0.3466435968875885\n",
      "10000 0.3466399610042572\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "\n",
    "for step in range(10001):\n",
    "    # forward\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "    # BCE\n",
    "    cost = -torch.mean(Y*torch.log(Y_pred) + (1-Y) * torch.log(1-Y_pred))\n",
    "    \n",
    "    # Back prop (chain rule) (backward)\n",
    "    # loss derivative\n",
    "    d_Y_pred = (Y_pred-Y) / (Y_pred * (1.0-Y_pred) + 1e-7)\n",
    "    \n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
    "    \n",
    "    # Layer 1\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
    "    \n",
    "    # weight update (step)\n",
    "    w1 = w1 - lr*d_w1\n",
    "    b1 = b1 - lr*torch.mean(d_b1, 0)\n",
    "    w2 = w2 - lr*d_w2\n",
    "    b2 = b2 - lr*torch.mean(d_b2, 0)\n",
    "    \n",
    "    if step%1000==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: xor-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.715934157371521\n",
      "300 0.596375584602356\n",
      "600 0.3768468201160431\n",
      "900 0.35934001207351685\n",
      "1200 0.3544251024723053\n",
      "1500 0.35218489170074463\n",
      "1800 0.3509174585342407\n",
      "2100 0.35010701417922974\n",
      "2400 0.34954583644866943\n",
      "2700 0.34913522005081177\n",
      "3000 0.34882205724716187\n",
      "3300 0.3485758304595947\n",
      "3600 0.3483772873878479\n",
      "3900 0.348213791847229\n",
      "4200 0.3480769991874695\n",
      "4500 0.34796082973480225\n",
      "4800 0.34786105155944824\n",
      "5100 0.3477745056152344\n",
      "5400 0.34769824147224426\n",
      "5700 0.34763121604919434\n",
      "6000 0.34757161140441895\n",
      "6300 0.34751802682876587\n",
      "6600 0.3474700450897217\n",
      "6900 0.34742674231529236\n",
      "7200 0.34738701581954956\n",
      "7500 0.34735098481178284\n",
      "7800 0.34731805324554443\n",
      "8100 0.34728747606277466\n",
      "8400 0.3472594618797302\n",
      "8700 0.347233384847641\n",
      "9000 0.3472091555595398\n",
      "9300 0.3471869230270386\n",
      "9600 0.3471660315990448\n",
      "9900 0.34714627265930176\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "# nn Layers\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
    "# define cost/loss and optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%300==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: xor-nn-wide-deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7164230346679688\n",
      "100 0.6931554079055786\n",
      "200 0.6931540966033936\n",
      "300 0.6931527853012085\n",
      "400 0.693151593208313\n",
      "500 0.6931504607200623\n",
      "600 0.6931492686271667\n",
      "700 0.693148136138916\n",
      "800 0.6931470632553101\n",
      "900 0.6931459307670593\n",
      "1000 0.6931447982788086\n",
      "1100 0.6931437253952026\n",
      "1200 0.6931427121162415\n",
      "1300 0.6931416392326355\n",
      "1400 0.6931405067443848\n",
      "1500 0.6931394338607788\n",
      "1600 0.6931382417678833\n",
      "1700 0.6931371688842773\n",
      "1800 0.6931359767913818\n",
      "1900 0.6931347846984863\n",
      "2000 0.6931334733963013\n",
      "2100 0.6931322813034058\n",
      "2200 0.6931309103965759\n",
      "2300 0.6931295394897461\n",
      "2400 0.6931281089782715\n",
      "2500 0.6931265592575073\n",
      "2600 0.6931249499320984\n",
      "2700 0.6931232213973999\n",
      "2800 0.6931214332580566\n",
      "2900 0.6931195855140686\n",
      "3000 0.6931174397468567\n",
      "3100 0.693115234375\n",
      "3200 0.693112850189209\n",
      "3300 0.6931103467941284\n",
      "3400 0.6931076049804688\n",
      "3500 0.6931045055389404\n",
      "3600 0.6931012272834778\n",
      "3700 0.6930975914001465\n",
      "3800 0.6930936574935913\n",
      "3900 0.6930893659591675\n",
      "4000 0.6930843591690063\n",
      "4100 0.6930789947509766\n",
      "4200 0.6930727958679199\n",
      "4300 0.693065881729126\n",
      "4400 0.6930580139160156\n",
      "4500 0.6930490732192993\n",
      "4600 0.6930387020111084\n",
      "4700 0.6930266618728638\n",
      "4800 0.6930125951766968\n",
      "4900 0.6929957866668701\n",
      "5000 0.6929758787155151\n",
      "5100 0.692951500415802\n",
      "5200 0.69292151927948\n",
      "5300 0.6928839087486267\n",
      "5400 0.6928355693817139\n",
      "5500 0.692772388458252\n",
      "5600 0.6926865577697754\n",
      "5700 0.6925660371780396\n",
      "5800 0.6923881769180298\n",
      "5900 0.6921088695526123\n",
      "6000 0.6916301250457764\n",
      "6100 0.690697968006134\n",
      "6200 0.6884748935699463\n",
      "6300 0.6807767748832703\n",
      "6400 0.6191476583480835\n",
      "6500 0.07421649247407913\n",
      "6600 0.010309841483831406\n",
      "6700 0.004796213004738092\n",
      "6800 0.003011580090969801\n",
      "6900 0.0021596732549369335\n",
      "7000 0.001668647164478898\n",
      "7100 0.0013521360233426094\n",
      "7200 0.0011324126971885562\n",
      "7300 0.0009715152555145323\n",
      "7400 0.0008490675827488303\n",
      "7500 0.000752839376218617\n",
      "7600 0.0006754109635949135\n",
      "7700 0.0006118417950347066\n",
      "7800 0.000558803731109947\n",
      "7900 0.0005138349952176213\n",
      "8000 0.00047530914889648557\n",
      "8100 0.000441973126726225\n",
      "8200 0.00041282744496129453\n",
      "8300 0.0003871413064189255\n",
      "8400 0.0003643627860583365\n",
      "8500 0.0003439848660491407\n",
      "8600 0.00032572413329035044\n",
      "8700 0.0003092226979788393\n",
      "8800 0.0002942568971775472\n",
      "8900 0.0002806179691106081\n",
      "9000 0.0002681269543245435\n",
      "9100 0.00025669438764452934\n",
      "9200 0.0002461116237100214\n",
      "9300 0.0002363934472668916\n",
      "9400 0.00022736100072506815\n",
      "9500 0.0002190142695326358\n",
      "9600 0.00021117438154760748\n",
      "9700 0.0002038860257016495\n",
      "9800 0.00019705976592376828\n",
      "9900 0.00019065086962655187\n",
      "10000 0.00018467426707502455\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn Layers\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
    "# define cost/loss and optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%100==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-1 ReLU\n",
    "- ReLU\n",
    "- Sigmoid\n",
    "- Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of Sigmoid\n",
    "- Input -> Network -> Output\n",
    "- GD 계산시, 시그모이드 사용지 작은 값이 곱해지면서 Gradient Vanishing 이라고 함\n",
    "\n",
    "### ReLU\n",
    "- $f(x) = max(0, x)$\n",
    "\n",
    "```python\n",
    "torch.nn.Sigmoid()\n",
    "torch.nn.Tanh()\n",
    "torch.nn.ReLU()\n",
    "torch.nn.LeakyReLU()\n",
    "```\n",
    "\n",
    "### Optimizer in PyTorch\n",
    "```python\n",
    "torch.optim.SGD()\n",
    "torch.optim.Adadelta()\n",
    "torch.optim.RMSprop()\n",
    "...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-2 Weight initialization\n",
    "- 가중치 초기화(Wegight Initialization)\n",
    "- Xavier / He initialization\n",
    "\n",
    "### Why good initialization?\n",
    "- 가중치 초기화를 진행한 경우, 성능과 학습에 좋음\n",
    "- 단순히 0으로 초기화하는 경우, 오류역전파 과정에서 모든것이 0으로 초기화되는 문제가 있음\n",
    "- RBM을 이용해 가중치를 초기화\n",
    "\n",
    "### RBM (Restricted Boltzmann Machine)\n",
    "- 레이어 안에서는 연결이 없다\n",
    "- 다른 레이어 사이에는 FC\n",
    "\n",
    "### Pre-training\n",
    "1. x - (RBM) - h1\n",
    "2. x - h1 - (RBM) - h2\n",
    "3. x - h1 - h2 - (RBM) - y, h3\n",
    "4. Fine-tuning\n",
    "\n",
    "최근에는, `Xavier / He initialization`와 같이 RBM 보다 성능이 향상된 초기화 방법이 있음\n",
    "\n",
    "### Xavier / He initialization\n",
    "- RBM에서는 무작위로 가중치를 초기화\n",
    "- 위 두 방식은, 레이어에 따라 초기화하는 방식이 달라짐\n",
    "- Normal Distribution (정규분포)\n",
    "  - 가우스 분포\n",
    "  - 수집된 자료의 분포를 근사하는데 자주 사용\n",
    "- Uniform Distribution (연속균등분포)\n",
    "  - 연속 확률 분포, 분포가 특정 범위내에서 균등하게 나타나 있을 경우\n",
    "  - `[a, b]`\n",
    "\n",
    "#### Xavier\n",
    "1. Normal\n",
    "   1. $W \\sim N(0, Var(W))$\n",
    "   2. $Var(x) = \\sqrt {2 \\over n_{in} + n_{out}}$\n",
    "2. Uniform\n",
    "   1. $W \\sim U(-\\sqrt {6 \\over n_{in} + n_{out}}, + \\sqrt {6 \\over n_{in} + n_{out}})$\n",
    "\n",
    "#### He ubutuakuzatuib\n",
    "- `out`을 모두 제거\n",
    "\n",
    "1. Normal\n",
    "   1. $W \\sim N(0, Var(W))$\n",
    "   2. $Var(W) = \\sqrt {2 \\over n_{in}}$\n",
    "2. Uniform\n",
    "   1. $W \\sim U(-\\sqrt{6 \\over n_{in}}, +\\sqrt{6 \\over n_{in}})$\n",
    "\n",
    "```python\n",
    "# PyTorch Xavier 공식 구현 코드\n",
    "def xavier_uniform_(tensor, gain=1):\n",
    "    fan_in, fan_out = _calculate_fan_in_and_out(tensor) # 초기화하고자하는 인풋, 아웃풋 레이아웃 수\n",
    "    std = gain * math.sqrt(2.0 / (fan_in + fan_out))\n",
    "    a = math.sqrt(3.0) * std\n",
    "    with torch.no_grad():\n",
    "        return tensor.uniform_(-a, a)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-3 Dropout\n",
    "- 과최적화(Overfitting)\n",
    "- 드롭아웃(Dropout)\n",
    "\n",
    "### Dropout\n",
    "- 학습을 진행하면, 각 레이어에 존재하는 노드를 무작위로 껐다 켯다하면서 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "# nn Layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "drop_prob = 0.5 # dropout 확률\n",
    "dropout = torch.nn.Dropout(p=drop_prob)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(\n",
    "    linear1, relu, dropout,\n",
    "    linear2, relu, dropout,\n",
    "    linear3, relu, dropout,\n",
    "    linear4, relu, dropout,\n",
    "    linear5\n",
    ").to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # dropout 적용\n",
    "\n",
    "# dropout 미적용 (평가)\n",
    "with torch.no_grad():\n",
    "    model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-4 Batch Normalization\n",
    "- Batch Normalization\n",
    "- 경사 소실(Gradient Vanishin) / 폭발(Exploding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Vanishing / Exploding\n",
    "- Vanishing: 시그모이드 함수를 사용하는 경우, 층이 깊어 질수록 backpropagation이 잘 안되는 것\n",
    "- Exploding: Vanishing의 반대\n",
    "\n",
    "#### 해결 방법\n",
    "- 활성함수 변경\n",
    "- 초기화\n",
    "- 작은 learning rate\n",
    "- Batch Normalization\n",
    "\n",
    "### Internal Covariate Shift\n",
    "- Train과 Test의 분포가 달라서 발생하는 문제\n",
    "- 입력과 출력의 분포 차이를 줄이기 위해 정규화를 사용했지만, 각 레이어마다 분포가 달라질 수 있음\n",
    "\n",
    "### Batch Normalization\n",
    "- Input: Values of $x$ over a mini-batch: $\\beta = \\{x_1, ... , x_m\\}$\n",
    "- Parameters: $\\gamma, \\beta$\n",
    "- Ouput: $\\{y_i = BN_{\\gamma, \\beta} (x_i) \\}$\n",
    "\n",
    "- $\\mu \\leftarrow {1 \\over m} {\\sum_{i=1}^m} x_i $ // mini-batch mean\n",
    "- $\\sigma^2_\\beta \\leftarrow {1 \\over m} {\\sum_{i=1}^m}(x_i - \\mu\\beta)^2$ // mini-bath var\n",
    "- $\\hat x_i = {{x_i - \\mu\\beta} \\over {\\sqrt \\sigma^2_\\beta + \\epsilon}}$ // normalize\n",
    "- $y_i = \\gamma \\hat x_i + \\beta \\equiv BN_{\\gamma, \\beta}(x_i)$ // scale and shift\n",
    "\n",
    "\n",
    "$\\gamma$를 곱해주고 $\\beta$를 더해주는 과정을 통해, 활성 함수 이후 잃어버린 선형성을 보정\n",
    "\n",
    "### Train & eval mode\n",
    "- `Dropout`과 동일하게 `Batch Normalization`에도 적용해야 함\n",
    "- 활성함수 이전에 사용하는 경향이 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 A. DNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "#\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#\n",
    "outputs = model(imgs)\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6170c7e636c2dbd0f88cb8f557866518ac6c7d83ee4f2709c7df3161954bfcc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
