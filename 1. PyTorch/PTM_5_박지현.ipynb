{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ujhcR1Un1nLcE0MHzS4aoweRh9R-F0Nj",
      "authorship_tag": "ABX9TyMBL/8xe9nTKywC+1DweQDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nuyhc/RhythmStudy/blob/main/1.%20PyTorch/PTM_5_%EB%B0%95%EC%A7%80%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 엔드투엔드 \n",
        "\n",
        "* 10장 => 데이터를 읽는 - 1단계\n",
        "    * .MHD + .RAW\n",
        "* 11, 12장 => 결절을 분류하는 문제에 집중 - 4단계\n",
        "* 13장 => 다시 2단계(결절 후보들을 찾아 세그멘테이션 하기) - 2단계\n",
        "* 14장 => 3단계(그룹화)와 5단계(결절 분석 및 진단) - 3단계, 5단계\n",
        "\n",
        "데이터 로딩 -> 세그멘테이션 -> 그룹화 -> 분류 -> 결절 분석 및 진단\n",
        "\n",
        "[LUNA 데이터 사이트] https://luna16.grand-challenge.org/"
      ],
      "metadata": {
        "id": "-tWwIx_Lf-Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [PTM] 10장. 여러 데이터 소스를 통합 데이터셋으로 합치기\n",
        "\n",
        "* 원본 데이터 파일을 읽어 처리하기\n",
        "* 데이터를 표현하는 파이썬 클래스 구현\n",
        "* 데이터를 파이토치에서 사용 가능한 포맷으로 변환\n",
        "* 훈련 데이터와 검증 데이터의 시각화"
      ],
      "metadata": {
        "id": "XMDFTIUAdGsK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n5w7mlfcnVh",
        "outputId": "e9dfbdb2-ac80-44a9-afa1-1a8f03308f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dlwpt-code'...\n",
            "remote: Enumerating objects: 703, done.\u001b[K\n",
            "remote: Total 703 (delta 0), reused 0 (delta 0), pack-reused 703\u001b[K\n",
            "Receiving objects: 100% (703/703), 176.00 MiB | 17.51 MiB/s, done.\n",
            "Resolving deltas: 100% (309/309), done.\n",
            "Checking out files: 100% (228/228), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/deep-learning-with-pytorch/dlwpt-code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 원본 CT 데이터 파일\n",
        "\n",
        "CT 데이터\n",
        "* .mhd 파일: 메타데이터 헤더 정보 포함 파일\n",
        "* .raw 파일: 3차원 배열을 만들 원본 데이터 바이트를 포함 파일  \n",
        "각 파일 이름은 **시리즈 UID**(DICOM Digital Imaging and Communications in Medicals  명명법에 유래했다)라고 불리는 CT 스캔 단일 식별자로 시작한다.  \n",
        "예를 들어 시리즈 UID 1.2.3의 경우, 1.2.3.mhd와 1.2.3.raw의 두 가지 파일이 있다.  \n",
        "\n",
        "`일단, CT 데이터에 적용할 좌표계 변환이 있어야 한다 정도만 기억해두자`\n",
        "\n",
        "LUNA\n",
        "* 애노테이션 데이터: 각 결절의 좌표 목록, 악성 여부, 해당 CT 스캔의 시리즈 UID\n",
        "    * 결절 좌표가 좌표계 변환 정보를 거치면 결절의 중심에 해당하는 복셀의 인덱스, 행, 열 정보가 생김\n",
        "\n",
        "(I,R,C)좌표를 사용 -> CT 데이터의 작은 3차원 부분 단면을 얻어 모델에 대한 입력으로 사용 -> 3차원 배열과 함게, 훈련 샘플 튜플의 나머지를 구성해야 함 (샘플 배열, 결절의 상태 플래그, 시리즈 UID, 결절 후보군의 CT 리스트 중 이 샘플이 몇 번째 인덱스인지 등이 포함)  \n",
        "`파이토치가 Dataset 서브클래스를 통해 얻고자 하는 것이 정확히 이 튜플이다. 동시에 이 튜플을 원본 데이터를 표준 구조의 파이토치 텐서로 변환하는 과정의 마지막 부분이다.`"
      ],
      "metadata": {
        "id": "E_8oRE9hgX22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 LUNA 애노테이션 데이터 파싱\n",
        "LUNA에서 제공하는 csv 파일을 먼저 파싱하여 각 CT 스캔 중 관심 있는 부분을 파악해볼 필요가 있음\n",
        "\n",
        "얻을 수 있는 정보\n",
        "* 좌표 정보\n",
        "* 해당 좌표 지점이 결절인지 여부\n",
        "* CT 스캔에 대한 고유 식별자\n",
        "\n",
        "[LUNA 데이터 사이트] https://luna16.grand-challenge.org/"
      ],
      "metadata": {
        "id": "dg5wZwfZjlmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "candidates.csv 파일에는 조직 덩어리가 결절일 가능성이 있는지와 악성 종양 또는 양성 종양 여부, 그리고 그 밖에 정보가 들어있다. 추후 훈련, 검증 데이터셋으로 나눌 것이다."
      ],
      "metadata": {
        "id": "tFvyTNunoexm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일의 행 개수 카운트\n",
        "!wc -l /content/drive/MyDrive/리듬스터디/data/LUNA/candidates.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xabUzoIgj3x4",
        "outputId": "33747a9b-692c-4ede-d2d2-ff506346f2a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "551066 /content/drive/MyDrive/리듬스터디/data/LUNA/candidates.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 앞부분 일부를 출력, 첫 줄은 칼럼 헤더\n",
        "# .csv 파일의 첫 행은 열 헤어를 정의함\n",
        "!head /content/drive/MyDrive/리듬스터디/data/LUNA/candidates.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOaRQ6exmMWK",
        "outputId": "e05cd4a3-08ea-48c7-f1b4-3f2726cbd7f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seriesuid,coordX,coordY,coordZ,class\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-56.08,-67.85,-311.92,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,53.21,-244.41,-245.17,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,103.66,-121.8,-286.62,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-33.66,-72.75,-308.41,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-32.25,-85.36,-362.51,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-26.65,-203.07,-165.07,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-74.99,-114.79,-311.92,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-16.14,-248.61,-239.55,0\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,135.89,-141.41,-252.2,0\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "??? 리눅스 grep 명령어"
      ],
      "metadata": {
        "id": "JafUIHXMoB4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 결절이라서 1로 끝나는 행 개수 카운트\n",
        "!grep ',1$' /content/drive/MyDrive/리듬스터디/data/LUNA/candidates.csv | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR-g289tmkkk",
        "outputId": "449c9285-6de2-4efc-d40a-a1a18e841a67"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "annotations.csv 파일에는 결절로 플래그된 후보들에 대한 정보가 포함되어 있다. 특별히 `diameter_mm` 정보를 주목할 만하다. 대략 1,200개 결절에 대한 크기 정보가 있다. 이 유용한 정보를 결절 크기의 분포로 가정하여 훈련, 검증 데이터를 만드는 데 유용하게 사용할 수 있다."
      ],
      "metadata": {
        "id": "-i7se9feott1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l /content/drive/MyDrive/리듬스터디/data/LUNA/annotations.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZJtTFoFm9Zv",
        "outputId": "5099602c-75e7-4a5a-cd5d-21d12b6fd9ed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1187 /content/drive/MyDrive/리듬스터디/data/LUNA/annotations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head /content/drive/MyDrive/리듬스터디/data/LUNA/annotations.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTd48iCgo8wz",
        "outputId": "0dea5b60-77fc-4023-cd4d-4bae00b7bd4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seriesuid,coordX,coordY,coordZ,diameter_mm\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-128.6994211,-175.3192718,-298.3875064,5.651470635\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,103.7836509,-211.9251487,-227.12125,4.224708481\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793540579077826395208,69.63901724,-140.9445859,876.3744957,5.786347814\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405,-24.0138242,192.1024053,-391.0812764,8.143261683\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405,2.441546798,172.4648812,-405.4937318,18.54514997\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405,90.93171321,149.0272657,-426.5447146,18.20857028\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405,89.54076865,196.4051593,-515.0733216,16.38127631\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100953483028192176989979435275,81.50964574,54.9572186,-150.3464233,10.36232088\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.102681962408431413578140925249,105.0557924,19.82526014,-91.24725078,21.08961863\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2.1 훈련셋과 검증셋\n",
        "supervised learning 작업은 데이터를 training set, validation set으로 나눈다.  \n",
        "일단 크기 순으로 정렬한 후 매 N번째에 대해 검증셋을 넣어 분포를 반영한 검증셋을 구성하자.\n",
        "* 안타깝게도 annotations.csv에서 제공하는 위치 정보는 candidates.csv의 좌표와 정확하게 일치하지는 않는다."
      ],
      "metadata": {
        "id": "xPqYCFQApY9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep 100225287222365663678666836860 /content/drive/MyDrive/리듬스터디/data/LUNA/annotations.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__ZkwWk0pBPU",
        "outputId": "3f3bf27b-358a-40cf-e27f-84ee8093e2c0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,-128.6994211,-175.3192718,-298.3875064,5.651470635\r\n",
            "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860,103.7836509,-211.9251487,-227.12125,4.224708481\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep '100225287222365663678666836860.*,1$' /content/drive/MyDrive/리듬스터디/data/LUNA/candidates.csv"
      ],
      "metadata": {
        "id": "Z9uxtO5qqN88"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2.2 애노테이션 데이터와 후보 데이터 합치기\n",
        "annotations.csv와 candidates.csv의 정보를 합치는 `getCandidateInfoList` 함수를 만들어보자.  \n",
        "각 결절 정보를 담아둘 `named tuple`을 파일 상단에 두고 사용하자"
      ],
      "metadata": {
        "id": "1zevZkT4qlQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "CandidateInfoTuple = namedtuple(\n",
        "    'CandidateInfoTuple',\n",
        "    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
        ")"
      ],
      "metadata": {
        "id": "kdjg11pCqZxa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 튜플은 우리가 필요로 하는 CT 데이터가 빠져 있으니 훈련 샘플이 아니다.  \n",
        "이 데이터는 전문가의 애노테이션 데이터에 대해 나름대로 깔끔하게 다듬어진 통합 인터페이스를 나타낸다. \n",
        "* 모델 훈련 작업과 지저분한 데이터를 처리하는 작업을 분리하는 것이 매우 중요(그렇지 않으면 훈련 루프가 금세 지저분해짐)"
      ],
      "metadata": {
        "id": "Ys9bZAxBrZ7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`NoduleInfoTuple` 인스턴스 리스트를 만드는 함수는 in-memory caching decorator를 사용하고 디스크 파일 경로를 얻는다."
      ],
      "metadata": {
        "id": "L_g5qYHKsTXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import csv\n",
        "import functools\n",
        "import glob\n",
        "import os\n",
        "\n",
        "@functools.lru_cache(1) # 표준 인메모리 캐싱 라이브러리\n",
        "def getCandidateInfoList(requireOnDisk_bool=True):  # requireOnDisk_bool은 디스크에 없는 데이터는 걸러내기 위함\n",
        "    mhd_list = glob.glob('data-unversioned/part2/luna/subset*/*.mhd')\n",
        "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST95_7uurJdW",
        "outputId": "c74221ae-0fe7-4caa-b861-e0cc1a09c0d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `1'\n",
            "/bin/bash: -c: line 0: `functools.lru_cache(1)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "후보 정보를 얻었다면 annotations.csv의 직경 정보를 합치자.  \n",
        "먼저 애노테이션 정보는 series_uid로 그룹화해서 두 파일에서 일치하는 행을 찾아내는 키로 사용하자"
      ],
      "metadata": {
        "id": "C2A7jcxluUSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    diameter_dict = {}\n",
        "    with open('data/part2/luna/annotations.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "            annotationDiameter_mm = float(row[4])\n",
        "\n",
        "            diameter_dict.setdefault(series_uid, []).append(\n",
        "                (annotationCenter_xyz, annotationDiameter_mm)\n",
        "            )"
      ],
      "metadata": {
        "id": "AKKkEqLGstNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리고 이제 candidates.csv의 정보를 사용하여 전체 후보 리스트를 만들자"
      ],
      "metadata": {
        "id": "ZOvmbnJXuskh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    candidateInfo_list = []\n",
        "    with open('data/part2/luna/candidates.csv', \"r\") as f:\n",
        "        for row in list(csv.reader(f))[1:]:\n",
        "            series_uid = row[0]\n",
        "\n",
        "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
        "                continue\n",
        "\n",
        "            isNodule_bool = bool(int(row[4]))\n",
        "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
        "\n",
        "            candidateDiameter_mm = 0.0\n",
        "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
        "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
        "                for i in range(3):\n",
        "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
        "                    if delta_mm > annotationDiameter_mm / 4:\n",
        "                        break\n",
        "                else:\n",
        "                    candidateDiameter_mm = annotationDiameter_mm\n",
        "                    break\n",
        "\n",
        "            candidateInfo_list.append(CandidateInfoTuple(\n",
        "                isNodule_bool,\n",
        "                candidateDiameter_mm,\n",
        "                series_uid,\n",
        "                candidateCenter_xyz,\n",
        "            ))"
      ],
      "metadata": {
        "id": "FQkxY1ZnurV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 데이터를 정렬 후 반환한다"
      ],
      "metadata": {
        "id": "BsbRreGnvZqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    candidateInfo_list.sort(reverse=True)\n",
        "    return candidateInfo_list"
      ],
      "metadata": {
        "id": "YPw5T4otvdqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "noduleInfo_list의 튜플 멤버 순서를 이 정렬로 만들어진다. 이렇게 데이터를 정렬하면 CT 단면들을 모아 결절 직경에 대해 잘 분포된 실제 결절을 반영하는 덩어리를 얻어올 수 있게 된다."
      ],
      "metadata": {
        "id": "wxj4MN68viLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3 개별 CT 스캔 로딩\u001c\n",
        "디스크에서 CT 데이터를 얻어와 파이썬 객체로 변환해서 3차원 결절 밀도 데이터로 사용할 수 있도록 만드는 작업이다. 결졀 애노테이션 정보는 원본 데이터에서 얻어내고자 하는 영역에 대한 map이라고 생각하면 된다. \n",
        "* CT 스캔을 읽어 복셀 배열을 만든 후 환자 좌표를 배열 인덱스로 변환하는 부분\n",
        "\n",
        "CT 스캔 파일의 원래 포맷은 DICOM(www.dicomstandard.org) 이라고 부른다. DICOM 표준의 최초 버전은 1984년에 만들어졌다\n",
        " * LUNA는 우리가 사용하려는 데이터를 MetalO(https://itk.org/Wiki/MetalO/Documentation#Quick_Start) 포맷으로 변환해 놓았고 사용하기도 쉽다.\n",
        " * 데이터 파일 포맷을 블랙박스로 간주하고 친숙한 넘파이 배열로 읽어들이기 위해 SimpleITK를 사용할 것이다."
      ],
      "metadata": {
        "id": "L6Pa0g5-vx8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx15xRYqxmAa",
        "outputId": "d4ab3d86-05cd-4fee-b712-632557fc5f17"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "\n",
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            'data-unversioned/part2/luna/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)"
      ],
      "metadata": {
        "id": "My9Wn4P5vuKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.1 하운스필드 단위"
      ],
      "metadata": {
        "id": "5LBfxnnyyB5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct_a.clip(-1000, 1000, ct_a)"
      ],
      "metadata": {
        "id": "nDqt4_ODyHkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.series_uid = series_uid\n",
        "self.hu_a = ct_a"
      ],
      "metadata": {
        "id": "kM0fsPAKyL3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.4 환자 좌표계를 사용해 결절 위치 정하기"
      ],
      "metadata": {
        "id": "wTXlLfh2yU4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4.1 환자 좌표계"
      ],
      "metadata": {
        "id": "jT9G8do_yZCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4.2 CT 스캔 형태와 복셀 크기"
      ],
      "metadata": {
        "id": "yRaMVeneyera"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4.3 밀리리터를 복셀 주소로 변환하기\n",
        "1. 좌표를 XYZ 체계로 만들기 위해 IRC에서 CRI로 뒤집는다.\n",
        "2. 인덱스를 복셀 크기로 확대축소한다.\n",
        "3. 파이썬의 @를 사용하여 방향을 나타내는 행렬과 행렬곱을 수행한다.\n",
        "4. 기준으로부터 오프셋을 더한다."
      ],
      "metadata": {
        "id": "Z9rw4d4tyi9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
        "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
        "\n",
        "def irc2xyz(coord_irx, origin_xyz, vxSize_xyz, directions_a):\n",
        "    cri_a = np.array(coord_irc)[::-1]. # 넘파이 배열로 변환하며 순서를 바꾼다.\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coords_xyz = (direction_a @ (cri_a * vxSize_xyz)) + origin_a  # 2,3,4단계를 한 줄로 실행한다.\n",
        "    return XyzTuple(*coords_xyz)\n",
        "\n",
        "\n",
        "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
        "    origin_a = np.array(origin_xyz)\n",
        "    vxSize_a = np.array(vxSize_xyz)\n",
        "    coord_a = np.array(coord_xyz)\n",
        "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a  # 4,3,2단계를 실행한다.\n",
        "    cri_a = np.round(cri_a)  # 정수로 변환하기 전에 적절히 반올림해준다.\n",
        "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))  # 섞으면서 정수로 변환한다."
      ],
      "metadata": {
        "id": "56EOLIX9y1_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vyZZ4C281THT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ct:\n",
        "    def __init__(self, series_uid):\n",
        "        mhd_path = glob.glob(\n",
        "            'data-unversioned/part2/luna/subset*/{}.mhd'.format(series_uid)\n",
        "        )[0]\n",
        "\n",
        "        ct_mhd = sitk.ReadImage(mhd_path)\n",
        "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
        "\n",
        "        ct_a.clip(-1000, 1000, ct_a)\n",
        "\n",
        "        self.series_uid = series_uid\n",
        "        self.hu_a = ct_a\n",
        "\n",
        "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
        "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
        "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)"
      ],
      "metadata": {
        "id": "acDQ4s4l1K7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4.4 CT 스캔에서 결절 추출하기"
      ],
      "metadata": {
        "id": "R638IsTt1Ka0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def getRawCandidate(self, center_xyz, width_irc):\n",
        "        center_irc = xyz2irc(\n",
        "            center_xyz,\n",
        "            self.origin_xyz,\n",
        "            self.vxSize_xyz,\n",
        "            self.direction_a,\n",
        "        )\n",
        "\n",
        "        slice_list = []\n",
        "        for axis, center_val in enumerate(center_irc):\n",
        "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
        "            end_ndx = int(start_ndx + width_irc[axis])\n",
        "\n",
        "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
        "\n",
        "            if start_ndx < 0:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                start_ndx = 0\n",
        "                end_ndx = int(width_irc[axis])\n",
        "\n",
        "            if end_ndx > self.hu_a.shape[axis]:\n",
        "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
        "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
        "                end_ndx = self.hu_a.shape[axis]\n",
        "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
        "\n",
        "            slice_list.append(slice(start_ndx, end_ndx))\n",
        "\n",
        "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
        "\n",
        "        return ct_chunk, center_irc"
      ],
      "metadata": {
        "id": "hFfXXchE1eM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.5 간단한 데이터셋 구현"
      ],
      "metadata": {
        "id": "C4udU3JZ1qCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from util.disk import getCache\n",
        "from util.util import XyzTuple, xyz2irc\n",
        "from util.logconf import logging"
      ],
      "metadata": {
        "id": "eDyPA-sL2F_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def __len__(self):\n",
        "        return len(self.candidateInfo_list)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        return (\n",
        "            candidate_t,\n",
        "            pos_t,\n",
        "            candidateInfo_tup.series_uid,\n",
        "            torch.tensor(center_irc),\n",
        "        )"
      ],
      "metadata": {
        "id": "ZkjelJ4J15lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zv5pZ7Dj27vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def __getitem__(self, ndx):\n",
        "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
        "        width_irc = (32, 48, 48)\n",
        "\n",
        "        candidate_a, center_irc = getCtRawCandidate(\n",
        "            candidateInfo_tup.series_uid,\n",
        "            candidateInfo_tup.center_xyz,\n",
        "            width_irc,\n",
        "        )"
      ],
      "metadata": {
        "id": "bJxDVwuK27Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AyI2dq1u26yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        candidate_t = torch.from_numpy(candidate_a)\n",
        "        candidate_t = candidate_t.to(torch.float32)\n",
        "        candidate_t = candidate_t.unsqueeze(0)"
      ],
      "metadata": {
        "id": "wvi4FlgJ3ILm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        pos_t = torch.tensor([\n",
        "                not candidateInfo_tup.isNodule_bool,\n",
        "                candidateInfo_tup.isNodule_bool\n",
        "            ],\n",
        "            dtype=torch.long,\n",
        "        )"
      ],
      "metadata": {
        "id": "MLSHCmfw3LGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dx7v10J73QtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LunaDataset()[0]"
      ],
      "metadata": {
        "id": "FqcdkuNv3RNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.1 getCtRawCandidate 함수로 후보 배열 캐싱하기"
      ],
      "metadata": {
        "id": "jdbXKfI53eVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.lru_cache(1, typed=True)\n",
        "def getCt(series_uid):\n",
        "    return Ct(series_uid)\n",
        "\n",
        "@raw_cache.memoize(typed=True)\n",
        "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
        "    ct = getCt(series_uid)\n",
        "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
        "    return ct_chunk, center_irc"
      ],
      "metadata": {
        "id": "Jd84Kcov3l3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.2 LunaDataset.__init__으로 데이터셋 만들기"
      ],
      "metadata": {
        "id": "anHm7Ljy3oVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LunaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 val_stride=0,\n",
        "                 isValSet_bool=None,\n",
        "                 series_uid=None,\n",
        "            ):\n",
        "        self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
        "\n",
        "        if series_uid:\n",
        "            self.candidateInfo_list = [\n",
        "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
        "            ]"
      ],
      "metadata": {
        "id": "TSQy-GEj3v5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.3 훈련/검증 분리"
      ],
      "metadata": {
        "id": "1VlYTzMB3xP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        if isValSet_bool:\n",
        "            assert val_stride > 0, val_stride\n",
        "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list\n",
        "        elif val_stride > 0:\n",
        "            del self.candidateInfo_list[::val_stride]\n",
        "            assert self.candidateInfo_list"
      ],
      "metadata": {
        "id": "ttdkrs-533gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from p2ch10.dsets import getCandidateInfoList, getCt, LunaDataset\n",
        "candidateInfo_list = getCandidateInfoList(requireOnDisk_bool=False)\n",
        "positiveInfo_list = [x for x in candidateInfo_list if x[0]]\n",
        "diameter_list = [x[1] for x in positiveInfo_list]"
      ],
      "metadata": {
        "id": "TDrWbvB_38BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.4 데이터 렌더링"
      ],
      "metadata": {
        "id": "Y0hXeNoS4fnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from p2ch10.vis import findPositiveSamples, showCandidate\n",
        "positiveInfo_list = findPositiveSamples()"
      ],
      "metadata": {
        "id": "BYQkJIz-4i35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [PTM] 11장. 종양 탐지를 위한 분류 모델 훈련\n",
        "\n",
        "* 파이토치 DataLoader로 데이터 로딩하기\n",
        "* CT 데이터에서 분류를 수행하는 모델 만들기\n",
        "* 애플리케이션 기본 구조 설정\n",
        "* 메트릭 로깅과 표시"
      ],
      "metadata": {
        "id": "7E9Ln4x144ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.1 기본 모델과 훈련 루프\n",
        "\n",
        "구현의 기본 구조\n",
        "* 모델을 초기화하고 데이터를 로딩한다.\n",
        "* 어느 정도 임의로 선택한 에포크 수로 루프를 반복한다.\n",
        "    * LunaDataset이 반환한 훈련 데이터의 배치 루프를 돈다.\n",
        "    * 백그라운드에서 데이터로더 worker 프로세스는 적합한 배치를 읽어들인다.\n",
        "    * batch를 분류 모델에 전달하여 결과를 얻는다.\n",
        "    * 추정 결과를 실측 데이터와 비교하여 손실을 계산한다.\n",
        "    * 임시 데이터 구조에 모델의 성능 메트릭을 기록한다.\n",
        "    * 오차 역전파로 모델 가중치를 조정한다.\n",
        "    * 검증 데이터 배치로 루프 반복\n",
        "    * 검증 데이터 배치를 읽어들인다.\n",
        "    * 배치를 분루하고 손실을 계산한다.\n",
        "    * 모델이 검증 데이터에 대해 얼마나 잘 동작했는지를 기록한다.\n",
        "    * 매 에포크마다 진행 상황과 성능 정보를 출력한다."
      ],
      "metadata": {
        "id": "2Iq2_Q4w5MSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.2 애플리케이션의 메인 진입점"
      ],
      "metadata": {
        "id": "3IXtBoWu56G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "from util.util import importstr\n",
        "from util.logconf import logging\n",
        "log = logging.getLogger('nb')\n",
        "\n",
        "def run(app, *argv):\n",
        "    argv = list(argv)\n",
        "    argv.insert(0, '--num-workers=4')  # 4코어 8스레드 CPU로 가정했는데 수정 가능\n",
        "    log.info(\"Running: {}({!r}).main()\".format(app, argv))\n",
        "    \n",
        "    app_cls = importstr(*app.rsplit('.', 1))  # __import__보다 깔끔한 호출 방식\n",
        "    app_cls(argv).main()\n",
        "    \n",
        "    log.info(\"Finished: {}.{!r}).main()\".format(app, argv))"
      ],
      "metadata": {
        "id": "oCTpwsMz4ya6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    LunaTrainingApp().mane()"
      ],
      "metadata": {
        "id": "Xf5KcpFrAe98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LunaTrainingApp:\n",
        "    def __init__(self, sys_argv=None):\n",
        "        if sys_argv is None:\n",
        "            sys_argv = sys.argv[1:]\n",
        "\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument('--batch-size',\n",
        "            help='Batch size to use for training',\n",
        "            default=32,\n",
        "            type=int,\n",
        "        )\n",
        "        parser.add_argument('--num-workers',\n",
        "            help='Number of worker processes for background data loading',\n",
        "            default=8,\n",
        "            type=int,\n",
        "        )\n",
        "        parser.add_argument('--epochs',\n",
        "            help='Number of epochs to train for',\n",
        "            default=1,\n",
        "            type=int,\n",
        "        )\n",
        "        parser.add_argument('--balanced',\n",
        "            help=\"Balance the training data to half positive, half negative.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "        parser.add_argument('--augmented',\n",
        "            help=\"Augment the training data.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "        parser.add_argument('--augment-flip',\n",
        "            help=\"Augment the training data by randomly flipping the data left-right, up-down, and front-back.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "        parser.add_argument('--augment-offset',\n",
        "            help=\"Augment the training data by randomly offsetting the data slightly along the X and Y axes.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "        parser.add_argument('--augment-scale',\n",
        "            help=\"Augment the training data by randomly increasing or decreasing the size of the candidate.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "        parser.add_argument('--augment-rotate',\n",
        "            help=\"Augment the training data by randomly rotating the data around the head-foot axis.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "        parser.add_argument('--augment-noise',\n",
        "            help=\"Augment the training data by randomly adding noise to the data.\",\n",
        "            action='store_true',\n",
        "            default=False,\n",
        "        )\n",
        "\n",
        "        parser.add_argument('--tb-prefix',\n",
        "            default='p2ch12',\n",
        "            help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n",
        "        )\n",
        "        parser.add_argument('comment',\n",
        "            help=\"Comment suffix for Tensorboard run.\",\n",
        "            nargs='?',\n",
        "            default='dlwpt',\n",
        "        )\n",
        "\n",
        "        self.cli_args = parser.parse_args(sys_argv)\n",
        "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "\n",
        "        self.trn_writer = None\n",
        "        self.val_writer = None\n",
        "        self.totalTrainingSamples_count = 0\n",
        "\n",
        "        self.augmentation_dict = {}\n",
        "        if self.cli_args.augmented or self.cli_args.augment_flip:\n",
        "            self.augmentation_dict['flip'] = True\n",
        "        if self.cli_args.augmented or self.cli_args.augment_offset:\n",
        "            self.augmentation_dict['offset'] = 0.1\n",
        "        if self.cli_args.augmented or self.cli_args.augment_scale:\n",
        "            self.augmentation_dict['scale'] = 0.2\n",
        "        if self.cli_args.augmented or self.cli_args.augment_rotate:\n",
        "            self.augmentation_dict['rotate'] = True\n",
        "        if self.cli_args.augmented or self.cli_args.augment_noise:\n",
        "            self.augmentation_dict['noise'] = 25.0\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "\n",
        "    def initModel(self):\n",
        "        model = LunaModel()\n",
        "        if self.use_cuda:\n",
        "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                model = nn.DataParallel(model)\n",
        "            model = model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
        "        # return Adam(self.model.parameters())\n",
        "\n",
        "    def initTrainDl(self):\n",
        "        train_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            ratio_int=int(self.cli_args.balanced),\n",
        "            augmentation_dict=self.augmentation_dict,\n",
        "        )\n",
        "\n",
        "        batch_size = self.cli_args.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=self.cli_args.num_workers,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True,\n",
        "        )\n",
        "\n",
        "        batch_size = self.cli_args.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=self.cli_args.num_workers,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "        return val_dl\n",
        "\n",
        "    def initTensorboardWriters(self):\n",
        "        if self.trn_writer is None:\n",
        "            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
        "\n",
        "            self.trn_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\n",
        "            self.val_writer = SummaryWriter(\n",
        "                log_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))"
      ],
      "metadata": {
        "id": "wzT5tWv9A2L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.3 사전 훈련 설정과 초기화"
      ],
      "metadata": {
        "id": "Ixneo8L7BSLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3.1 모델과 옵티마이저 초기화"
      ],
      "metadata": {
        "id": "b5wVCpv1BVWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LunaTrainingApp:\n",
        "    def __init__(self, sys_argv=None):\n",
        "        \n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "\n",
        "    def initModel(self):\n",
        "        model = LunaModel()\n",
        "        if self.use_cuda:\n",
        "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1:  # 복수 개의 GPU를 탐지\n",
        "                model = nn.DataParallel(model)  # 모델을 래핑\n",
        "            model = model.to(self.device)  # GPU에 모델 파라미터 전달\n",
        "        return model\n",
        "\n",
        "\n",
        "    def initOptimizer(self):\n",
        "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
        "        # return Adam(self.model.parameters())"
      ],
      "metadata": {
        "id": "czPnNtUgBRuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3.2 데이터 로더의 관리와 데이터 공급"
      ],
      "metadata": {
        "id": "PUg4COLbCDEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def initTrainDl(self):\n",
        "        train_ds = LunaDataset(  # 커스텀 데이터셋\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "            ratio_int=int(self.cli_args.balanced),\n",
        "            augmentation_dict=self.augmentation_dict,\n",
        "        )\n",
        "\n",
        "        batch_size = self.cli_args.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(  # 바로 사용하면 되는 클래스\n",
        "            train_ds,\n",
        "            batch_size=batch_size,  # 알아서 배치로 나뉜다.\n",
        "            num_workers=self.cli_args.num_workers,\n",
        "            pin_memory=self.use_cuda,  # 고정된 메모리 영역이 GPU 쪽으로 빠르게 전송된다.\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "    \n",
        "\n",
        "    def main(self):\n",
        "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()  # 검증 데이터 로더는 훈련 데이터 로더와 매우 유사하다."
      ],
      "metadata": {
        "id": "yMxh8sdqD-Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.4 첫 번째 경로 신경망 설계"
      ],
      "metadata": {
        "id": "vH8yNQXgEVSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4.1 핵심 컨볼루션"
      ],
      "metadata": {
        "id": "-EkDiWaQEY5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from torch import nn as nn\n",
        "\n",
        "from util.logconf import logging\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "# log.setLevel(logging.WARN)\n",
        "# log.setLevel(logging.INFO)\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "\n",
        "class LunaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, conv_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        block_out = self.conv1(input_batch)\n",
        "        block_out = self.relu1(block_out)\n",
        "        block_out = self.conv2(block_out)\n",
        "        block_out = self.relu2(block_out)\n",
        "\n",
        "        return self.maxpool(block_out)\n"
      ],
      "metadata": {
        "id": "AQyq6Ji0Eaw8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4.2 전체 모델"
      ],
      "metadata": {
        "id": "Q7SCw4EVEmDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LunaModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, conv_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
        "\n",
        "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
        "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
        "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
        "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
        "\n",
        "        self.head_linear = nn.Linear(1152, 2)\n",
        "        self.head_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self._init_weights()"
      ],
      "metadata": {
        "id": "U9jvIbMNEu74"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 복잡한 문제: 컨볼루션을 선형으로 변환하기\n",
        "\n"
      ],
      "metadata": {
        "id": "kVbr6EdIEv3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def forward(self, input_batch):\n",
        "        bn_output = self.tail_batchnorm(input_batch)\n",
        "\n",
        "        block_out = self.block1(bn_output)\n",
        "        block_out = self.block2(block_out)\n",
        "        block_out = self.block3(block_out)\n",
        "        block_out = self.block4(block_out)\n",
        "\n",
        "        conv_flat = block_out.view(\n",
        "            block_out.size(0),\n",
        "            -1,\n",
        "        )\n",
        "        linear_output = self.head_linear(conv_flat)\n",
        "\n",
        "        return linear_output, self.head_softmax(linear_output)"
      ],
      "metadata": {
        "id": "uLJ0Zy8xE9-M"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 초기화\n",
        "\n"
      ],
      "metadata": {
        "id": "nndsMa8WE_tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) in {\n",
        "                nn.Linear,\n",
        "                nn.Conv3d,\n",
        "                nn.Conv2d,\n",
        "                nn.ConvTranspose2d,\n",
        "                nn.ConvTranspose3d,\n",
        "            }:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)"
      ],
      "metadata": {
        "id": "2Q7c9vuCE-sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.5 모델 훈련과 검증"
      ],
      "metadata": {
        "id": "w0_wG-bFFI7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(self):\n",
        "    for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
        "        trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "\n",
        "    def doTraining(self, epoch_ndx, train_dl):\n",
        "        self.model.train()\n",
        "        train_dl.dataset.shuffleSamples()\n",
        "        trnMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(train_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss_var = self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                train_dl.batch_size,\n",
        "                trnMetrics_g,\n",
        "            )\n",
        "\n",
        "            loss_var.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')"
      ],
      "metadata": {
        "id": "l-wbocCAFRoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5.1 computeBatchLoss 함수"
      ],
      "metadata": {
        "id": "Wf51nLk7FsNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
        "    input_t, label_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "    input_g = input_t.to(self.device, non_blocking=True)\n",
        "    label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "    logits_g, probability_g = self.model(input_g)\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_g = loss_func(\n",
        "        logits_g,\n",
        "        label_g[:,1],\n",
        "    )\n",
        "    start_ndx = batch_ndx * batch_size\n",
        "    end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "    metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1]\n",
        "    metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
        "    metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
        "\n",
        "    return loss_g.mean()"
      ],
      "metadata": {
        "id": "Tc5MtpHSF0Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS_LABEL_NDX=0\n",
        "METRICS_PRED_NDX=1\n",
        "METRICS_LOSS_NDX=2\n",
        "METRICS_SIZE = 3\n",
        "\n",
        "\n",
        "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
        "        input_t, label_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True)\n",
        "        label_g = label_t.to(self.device, non_blocking=True)\n",
        "\n",
        "        logits_g, probability_g = self.model(input_g)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
        "        loss_g = loss_func(\n",
        "            logits_g,\n",
        "            label_g[:,1],\n",
        "        )\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1]\n",
        "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
        "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
        "\n",
        "        return loss_g.mean()  # 전체 배치에 대한 손실값이다."
      ],
      "metadata": {
        "id": "5090PfgyF9EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5.2 훈련 때와 유사한 검증 루프"
      ],
      "metadata": {
        "id": "9nSSeurFGOhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(self):\n",
        "    for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
        "        valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "        \n",
        "    self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "\n",
        "\n",
        "def doValidation(self, epoch_ndx, val_dl):\n",
        "    with torch.no_grad():\n",
        "        self.model.eval()\n",
        "        valMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(val_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            val_dl,\n",
        "            \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                val_dl.batch_size,\n",
        "                valMetrics_g,\n",
        "            )\n",
        "\n",
        "    return valMetrics_g.to('cpu')"
      ],
      "metadata": {
        "id": "67TeFbZYGe1i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}