# Part-1 Basic ML
## Lab-01-1 Tensor Manipulation 1
- 텐서(Tensor)
- 넘파이(NumPy)
- 텐서 조작(Tensor Manipulation)
- 브로드캐스팅(Broadcasting)
### Tensor
- 2D Tensor: $|t| = (batch size, \ dim)$
- 3D Tensor: $|t| = (batch size, \ width, \ height)$ (vision)
- 3D Tensor: $|t| = (batch size, \ length, \ dim)$ (NLP, Seq.)
import torch
import numpy as np
#### 1D Array with NumPy
t = np.array([0., 1., 2., 3., 4., 5., 6.])
print(t)
print(f"Rank of t: {t.ndim}")
print(f"Shape of t: {t.shape}")
# Element
print(t[0], t[1], t[-1])
# Slicing
print(t[2:5], t[4:-1])
print(t[:2], t[3:])
#### 2D Array with NumPy
t = np.array(
    [[1., 2., 3.],
     [4., 5., 6.],
     [7., 8., 9.],
     [10., 11., 12.]]
)

print(t)
print(f"Rank of t: {t.ndim}")
print(f"Shape of t: {t.shape}")
#### 1D Array with PyTorch
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
# Rank
print(f"Rank: {t.dim()}")
# Shape
print(f"Shape: {t.shape}")
print(f"Shape: {t.size()}")
# Element
print(t[0], t[1], t[-1])
# Slicing
print(t[2:5], t[4:-1])
print(t[:2], t[3:])
#### 2D Array with PyTorch
t = torch.FloatTensor(
    [[1., 2., 3.],
     [4., 5., 6.],
     [7., 8., 9.],
     [10., 11., 12.]]
)

print(t)
# Rank
print(f"Rank: {t.dim()}")
# Shape
print(t.size())
# Slicing
print(t[:, 1])
print(t[:, 1].size())
print(t[:, :-1])
### Broadcasting
다른 크기의 행렬을 연산 할 때 적용되는 기능
# same shape
m1 = torch.FloatTensor([[3, 3]]) # (1, 2)
m2 = torch.FloatTensor([[2, 2]]) # (1, 2)

print(m1.shape, m2.shape)
print(m1+m2)
# Vec + scaler
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3])

print(m1.shape, m2.shape)
print(m1+m2)
# 2*1 vec + 1*2 vec
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3], [4]])

print(m1.shape, m2.shape)
print(m1+m2)
$$
\begin{bmatrix}
1 & 2
\end{bmatrix}
+
\begin{bmatrix}
3 \\
4
\end{bmatrix}
=
\begin{bmatrix}
1 & 2 \\
1 & 2
\end{bmatrix}
+
\begin{bmatrix}
3 & 3 \\
4 & 4
\end{bmatrix}
=
\begin{bmatrix}
4 & 5 \\
5 & 6
\end{bmatrix}
$$
### Multiplication vs Matrix Multiplication
- 딥러닝은 행렬곱을 굉장히 많이 사용하는 알고리즘
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])

print(f"Shape of Matrix 1: {m1.shape}") # 2 x 2
print(f"Shape of Matrix 2: {m2.shape}") # 2 x 1
print(m1)
print("-"*10)
print(m2)
m1.matmul(m2)
m1*m2
#### !!NOTE!!
- `np.matmul(a, b)` == `a@b`: 2D의 행렬곱
- `np.dot(a, b)`:
  - 1D의 내적
  - 2D의 행렬곱
  - nD의 경우, 첫 행렬의 마지막 축과 두 번째 행렬의 -2번째 축의 내적
a = np.array(
    [[1, 2],
     [3, 4]]
)

b = np.array(
    [[1, 2],
     [3, 4]]
)
np.matmul(a, b)
np.dot(a, b)
# point-wise
a*b
a = np.array([1, 2, 3])
b = np.array([1, 2, 3])
np.matmul(a, b), np.dot(a, b)
a*b
### Mean
t = torch.FloatTensor([1, 2])
print(t.mean())
t = torch.LongTensor([1, 2])
try: print(t.mean())
except Exception as e:
    print(e)
평균(Mean)은 정수형 텐서로는 못구함
t = torch.FloatTensor(
    [[1, 2],
     [3, 4]]
)
print(t)
### Sum
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)
print(t.sum()) # 10
print(t.sum(dim=0)) # 4, 6
print(t.sum(dim=1)) # 3, 7
### Max and Argmax
t.max() # 4
# Returns max and argmax
print(t.max(dim=0))
## Lab-01-2 Tensor Manipulation 2
- 텐서(Tensor)
- 넘파이(NumPy)
- 텐서 조작(Tensor Manipulation)
- View, Squeeze, UnSqueeze, Type Casting, Concatenate, Stacking, In-place Operation
### View (Reshape)
- `numpy.reshape`와 유사
t = np.array(
    [[[0, 1, 2],
      [3, 4, 5]],
     
     [[6, 7, 8],
      [9, 10, 11]]]
)

ft = torch.FloatTensor(t)

print(ft.shape)
print(ft.view([-1, 3]))
print(ft.view([-1, 3]).shape)
print(ft.view([-1, 1, 3]))
print(ft.view([-1, 1, 3]).shape)
### Squeeze
- 특정 차원의 엘리먼트가 1인 경우, 해당 차원을 지워줌
- `.squeeze(dim=?)`: ? 차원에 엘리먼트가 1인 경우가 있으면, 해당 차원을 지워줌
ft = torch.FloatTensor([[0], [1], [2]])
print(ft)
print(ft.shape)
print(ft.squeeze())
print(ft.squeeze().shape)
### Unsqueeze
- 차원은 추가
ft = torch.Tensor([0, 1, 2])

print(ft)
print(ft.shape)
print(ft.unsqueeze(0))
print(ft.unsqueeze(0).shape)
print(ft.view(1, -1))
print(ft.view(1, -1).shape)
print(ft.unsqueeze(1))
print(ft.unsqueeze(1).shape)
print(ft.view(-1, 1))
print(ft.view(-1, 1).shape)
### Type Casting
lt = torch.LongTensor([1, 2, 3, 4])
print(lt)
print(lt.float())
# 마스킹하는 경우 사용 가능
bt = torch.ByteTensor([True, False, False, True])
print(bt)
print(bt.long())
print(bt.float())
### Concatenate
- 이어 붙이기
x = torch.FloatTensor([[1, 2], [3, 4]])
y = torch.FloatTensor([[5, 6], [7, 8]])

print(torch.cat([x, y], dim=0))
print(torch.cat([x, y], dim=1))
### Stacking
x = torch.FloatTensor([1, 4])
y = torch.FloatTensor([2, 5])
z = torch.FloatTensor([3, 6])

print(torch.stack([x, y, z]))
print(torch.stack([x, y, z], dim=0))
print(torch.stack([x, y, z], dim=1))
### Ones and Zeros
x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
print(x)
print(torch.ones_like(x))
print(torch.zeros_like(x))
같은 디바이스에 텐서를 생성하게 됨
### In-place Opertaion
x = torch.FloatTensor([[1, 2], [3, 4]])

print(x.mul(2.))
print(x)
print(x.mul_(2.))
print(x)
## Lab-02 Linear regression
- 선형회귀(Linear Regression)
- 평균 제곱 오차(MSE)
- 경사하강법(Gradient Descent)
### Data Definition
| Hours(x) | Points(y) |
| :---: | :---: |
| 1 | 2 |
| 2 | 4 |
| 3 | 6 |
| 4 | ? |
# train data
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
### Hypothesis
$y = Wx + b$ <- 선형회귀
# weight
W = torch.zeros(1, requires_grad=True) 
# bias
b = torch.zeros(1, requires_grad=True)

hypothesis = x_train*W + b
`W`와 `b`를 0으로 초기화하고, 이를 학습 시키는 것이 목적  

학습을 위해 `requires_grad=True`로 학습할 것이라고 명시
### Compute loss
- Mean Squared Error (MSE)
  - $cost(W, b) = {1\over m} \sum^m_{i=1} (H(x^{(i)}) - y^{(i)})^2$  
cost = torch.mean((hypothesis-y_train)**2)
### Gradient Descent
1. `zero_grad()`로 gradient 초기화
2. `backward()`로 gradient 계산
3. `step`으로 개선 
optimizer = torch.optim.SGD([W, b], lr=0.01)

optimizer.zero_grad() # 1
cost.backward() # 2
optimizer.step() # 3
### Full Training Code
# train data
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

# weight
W = torch.zeros(1, requires_grad=True) 
# bias
b = torch.zeros(1, requires_grad=True)

optimizer = torch.optim.SGD([W, b], lr=0.01)

nb_epochs = 1000
for epoch in range(1, nb_epochs+1):
    hypothesis = x_train*W + b
    cost = torch.mean((hypothesis-y_train)**2)
    
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch%100==0: print(f"{cost.item():.3f}")
## Lab-03 Deeper Look at GD
- 가설 함수(Hypothesis Function)
- 평균 제곱 오차(MSE)
- 경사하강법(Gradient Descent)
# Simpler Hypothesis
hypothesis = x_train * W
# Dummpy Data
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
### Cost function: Intuition
- 실제값과 모델 값이 얼마나 다른지 나타내는 값 -> Cost
- 위 모델에서는, W=1일 때 cost=0

### Gradient Descent
- 목표는 cost 함수를 최소화하는 것 -> 미분 이용

$$
\nabla W = {\delta cost \over \delta W} = {{2 \over m} \sum^m_{i=1}(Wx^{(i)}-y^{(i)})x^{(i)}} \\
W : = W - \alpha \nabla W
$$

# Full Code
# Dummpy Data
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# 모델 초기화
W = torch.zeros(1)
lr = 0.1

nb_epochs = 10
for epoch in range(nb_epochs+1):
    # H(x) 계산
    hypothesis = x_train*W
    # cost gradient 계산
    cost = torch.mean((hypothesis - y_train)**2)
    gradient = torch.sum((W*x_train - y_train)*x_train)
    
    print(f"{epoch:04d}/{nb_epochs} W: {W.item():.3f}, Cost: {cost.item():.6f}")
    
    # 개선
    W -= lr*gradient
`torch.optim`으로도 gradient descent를 할 수 있음
- Optimizer 정의
- `optimizer.zero_grad()`로 gradient를 0으로 초기화
- `cost.backward()`로 gradient 계산
- `optimizer.step()`으로 gradient descent
# optimizer 설정
optimizer = torch.optim.SGD([W], lr=0.15)
# cost로 H(x) 개선
optimizer.zero_grad()
cost.backward()
optimizer.step()
from torch import optim

# Full Code
# Dummpy Data
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
# 모델 초기화
W = torch.zeros(1, requires_grad=True)
optimizer = optim.SGD([W], lr=0.15)

nb_epochs = 10
for epoch in range(nb_epochs+1):
    # H(x) 계산
    hypothesis = x_train*W
    # cost gradient 계산
    cost = torch.mean((hypothesis - y_train)**2)
    
    print(f"{epoch:04d}/{nb_epochs} W: {W.item():.3f}, Cost: {cost.item():.6f}")
    
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
## Lab-04-1 Multivariable Linear regression
- 다항 선형 회귀(Multivariable Linear regression)
- 가설 함수(Hypothesis Function)
- 평균 제곱 오차(MSE)
- 경사하강법(Gradient descent)
# Data
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

# 모델 초기화
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# optimizer
optimizer = optim.SGD([W, b], lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs+1):
    # H(x) 계산
    hypothesis = x_train@W # matmul
    # cost
    cost = torch.mean((hypothesis - y_train)**2)
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    print(f"Epoch: {epoch:4d}/{nb_epochs} hypothesis: {hypothesis.squeeze().detach()}, Cost: {cost.item():.6f}")
### nn.Module
import torch.nn as nn

class MultivariateLinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3, 1) # 입력 차원, 출력 차원
        
    def forward(self, x): # hypothesis 계산
        return self.linear(x)

    # gradient 계산은 토치가 알아서 해줌 (backward())

model = MultivariateLinearRegressionModel()    
hypothesis = model(x_train)
### F
- 다양한 코스트 함수를 사용 가능
import torch.nn.functional as F

# cost = F.mse_loss(pred, y_train)
# Full Code
# Data
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

# 모델 초기화
model = MultivariateLinearRegressionModel()

# optimizer
optimizer = optim.SGD(model.parameters(), lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs+1):
    # H(x) 계산
    hypothesis = model(x_train)
    # cost 계산
    cost = F.mse_loss(hypothesis, y_train)
    
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    print(f"Epoch: {epoch:4d}/{nb_epochs} hypothesis: {hypothesis.squeeze().detach()}, Cost: {cost.item():.6f}")
## Lab-04-2 Loading Data
- 다항 선형 회귀 (Multivariable Linear regression)
- 미니배치 경사하강법(Minibatch Gradient descnet)
- Dataset, DataLoader
### Minibatch Gradient Descent
- 전체 데이터를 균일하게 나눠서 학습
- 각 미니 배치의 코스트를 구하고 GD 수행
  - 업데이트가 빠름
  - 전체 데이터를 쓰지 않아서 잘못된 방향으로 업데이트를 할 수도 있음

### PyTorch Dataset
- 다음 메서드들은 필수로 구현되어야 함
  - `__len__()` : 데이터셋의 총 데이터 수
  - `__getitem__()` : 어떤 인덱스를 받았을 때, 그에 상응하는 입출력 데이터 반환
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self):
        self.x_data = [[73, 80, 75],
                       [93, 88, 93],
                       [89, 91, 90],
                       [96, 98, 100],
                       [73, 66, 70]]
        self.y_data = [[152], [185], [180], [196], [142]]
        
    def __len__(self):
        return len(self.x_data)
    
    def __getitem__(self, idx):
        x = torch.FloatTensor(self.x_data[idx])
        y = torch.FloatTensor(self.y_data[idx])
        return x, y
    
dataset = CustomDataset()
### PyTorch DataLoader
- 각 미니 배치의 크기를 지정해줘야 하고, 통상적으로 2의 제곱수를 이용
from torch.utils.data import DataLoader

dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
# Full Code
nb_epochs = 20
for epoch in range(nb_epochs+1):
    for batch_idx, samples in enumerate(dataloader):
        x_train, y_train = samples
        # H(x) 계산
        pred = model(x_train)
        # cost 계산
        cost = F.mse_loss(pred, y_train)
        # 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        
        print(f"Epoch {epoch:4d}/{nb_epochs} Batch {batch_idx+1}/{len(dataloader)} Cost: {cost.item():.6f}")
## Lab-05 Logistic Regression
- 로지스틱 회귀(Logistic Regression)
- 가설(Hypothesis)
- 손실함수(Cost Function)
- 평가(Evaluation)
### Logistic Regression
- Hypothesis $$H(x) = {1 \over 1 + e^{-W^TX}}$$
- Cost $$cost(W) = -{1 \over m} \sum ylog(H(x)) + (1-y)(log(1-H(x)))$$
- $H(x) = P(x=1;W) = 1 - P(x=0;W)$
- Weight Update via Gradient Descent
  - $W \ := W - \alpha {\delta \over \delta W} cost(W) \ = W - \alpha \nabla_W cost(W)$
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]

x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

print(x_train.shape)
print(y_train.shape)
### Computing the Hypothesis
W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# hypothesis = 1 / (1 + torch.exp(-(x_train@W +b)))
hypothesis = torch.sigmoid(x_train@W+b)
print(hypothesis)
print(hypothesis.shape)
### Computing the Cost Function
print(hypothesis)
print(y_train)
losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))
print(losses)
cost = losses.mean()
print(cost)
F.binary_cross_entropy(hypothesis, y_train)
# Train
optimizer = optim.SGD([W, b], lr=1)

nb_epochs = 1000
for epoch in range(nb_epochs+1):
    # Cost 계산
    hypothesis = torch.sigmoid(x_train@W+b)
    cost = F.binary_cross_entropy(hypothesis, y_train)
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch%100==0:
        print(f"Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item()}")
### Evaluation
hypothesis = torch.sigmoid(x_train@W+b)
print(hypothesis[:5])
pred = hypothesis >= torch.FloatTensor([0.5])
print(pred[:5])
pred[:5].float()
print(y_train[:5])
pred[:5].float() == y_train[:5]
### Higher Implementation with Class
class BinaryClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        return self.sigmoid(self.linear(x))
model = BinaryClassifier()

optimizer = optim.SGD(model.parameters(), lr=1)

nb_epochs = 100
for epoch in range(nb_epochs+1):
    # Cost 계산
    hypothesis = model(x_train)
    cost = F.binary_cross_entropy(hypothesis, y_train)
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch%10==0:
        pred = hypothesis >= torch.FloatTensor([0.5])
        correct_pred = pred.float()==y_train
        acc = correct_pred.sum().item() / len(correct_pred)
        print(f"Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f} ACC: {acc:2.2f}%")
## Lab-06 Softmax Classification
- 소프트맥스(Softmax)
- 크로스 엔트로피(Cross Entropy)
### Discrete Probability Distribution
- 이산 확률 분포

### Softmax
$$P(class=i) = {e^i \over \sum e^i}$$
z = torch.FloatTensor([1, 2, 3])
hypothesis = F.softmax(z, dim=0)
print(hypothesis)
hypothesis.sum()
### Cross Entropy
- 주어진 2개의 확률 분포가 얼마나 동일한지를 나타내는 수치
# Cross Entropy Loss (Low-level)
z = torch.rand(3, 5, requires_grad=True)
hypothesis = F.softmax(z, dim=1)
print(hypothesis)
y = torch.randint(5, (3, )).long()
print(y)
# One-Hot
y_one_hot = torch.zeros_like(hypothesis)
y_one_hot.scatter_(1, y.unsqueeze(1), 1)
cost = (y_one_hot*-torch.log(hypothesis)).sum(dim=1).mean()
print(cost)
torch.log(F.softmax(z, dim=1))
torch.log_softmax(z, dim=1)
F.nll_loss(F.log_softmax(z, dim=1), y)
# Cross Entropy = log_softmax + nll_loss
F.cross_entropy(z, y)
x_train = [[1, 2, 1, 1],
           [2, 1, 3, 2],
           [3, 1, 3, 4],
           [4, 1, 5, 5],
           [1, 7, 5, 5],
           [1, 2, 5, 6],
           [1, 6, 6, 6],
           [1, 7, 7, 7]]
y_train = [2, 2, 2, 1, 1, 1, 0, 0]

x_train = torch.FloatTensor(x_train)
y_train = torch.LongTensor(y_train)
# 모델 초기화
W = torch.zeros((4, 3), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([W, b], lr=0.1)

nb_epochs = 1000
for epoch in range(nb_epochs+1):
    # Cost 계산 1
    # hypothesis = F.softmax(x_train@W+b, dim=1)
    # y_one_hot = torch.zeros_like(hypothesis)
    # y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)
    # cost = (y_one_hot*-torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()
    
    # Cost 계산 2
    z = x_train@W+b
    cost = F.cross_entropy(z, y_train)
    
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch%100==0:
        print(f"Epoch: {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f}")

class SoftmaxClassifierModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(4, 3)
    def forward(self, x):
        return self.linear(x)
    
model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=0.1)

nb_epochs = 1000
for epoch in range(nb_epochs+1):
    # H(x) 계산
    pred = model(x_train)
    # cost 계산
    cost = F.cross_entropy(pred, y_train)
    # 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch%100==0:
        print(f"Epoch: {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f}")
## Lab-07-1 Tips
- 최대 가능도 추정(Maximum Likehood Estimation)
- 과적합(Overfitting)과 정규화(Regurlarization)
- 훈련 세트와 테스트 세트
- 학습률(Learning Rate)
- 데이터 전처리(Preprocessing)
### Maximum Likelihood Estimation (MLE)
- 최대 가능도 추정

$$
K \sim B(n, \theta)
\\
\begin{align}
P(K=k) = \begin{pmatrix} n \\ k \end{pmatrix} \theta^k (1-\theta)^{n-k}
\\
{n! \over k!(n-k)!} \theta^k(1-\theta)^{n-k}
\end{align}
$$

- Obseravation: n=100, k=27
- $\theta$가 궁금한것임 (y가 최대가 되는곳)
  - $\theta$ = 0.27
  - 관측값을 가장 잘 설명하는 확률 분포 함수의 파라미터 ($\theta$ 찾기)
- 기울기를 통해 찾을 수 있음 (Gradient Descent/Ascent)
- Descent -> Local Minimum
- Ascent -> Local Maximum

### Overfitting
- 최소화하는 것이 중요
- Train / Validation(Devlopment) / Test 이용
- More Data, Less Features, Regularization으로 해결 가능

#### Regularization
- Early Stopping
- Reducing Network Size
- Weight Decay
- Dropout
- Batch Normalization
x_train = torch.FloatTensor([[1, 2, 1],
                             [1, 3, 2],
                             [1, 3, 4],
                             [1, 5, 5],
                             [1, 7, 5],
                             [1, 2, 5],
                             [1, 6, 6],
                             [1, 7, 7]])
y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])

x_test = torch.FloatTensor([[2, 1, 1],
                            [3, 1, 2],
                            [3, 3, 4]])
y_test = torch.LongTensor([2, 2, 2])
class SoftmaxClassifierModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3, 3)
    def forward(self, x):
        return self.linear(x)
    
model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=0.1)

def train(model, optimizer, x_train, y_train):
    nb_epochs = 20
    for epoch in range(nb_epochs):
        pred = model(x_train)
        cost = F.cross_entropy(pred, y_train)
        
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        
        print(f"Epoch: {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f}")
        
def test(model, optimizer, x_test, y_test):
    pred = model(x_test)
    pred_classes = pred.max(1)[1]
    correct_count = (pred_classes==y_test).sum().item()
    cost = F.cross_entropy(pred, y_test)
    
    print(f"Acc: {correct_count/len(y_test)*100}% Cost: {cost.item():.6f}")
    

train(model, optimizer, x_train, y_train)
test(model, optimizer, x_test, y_test)
### Learning Rate
- 너무 크면 발산하면서 cost가 늘어남(overshooting)
model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=1e5)

train(model, optimizer, x_train, y_train)
- 너무 작으면 cost가 거의 줄어들지 않음
model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=1e-10)

train(model, optimizer, x_train, y_train)
- 적절한 숫자로 시작해, 발산하면 작게, 줄어들지 않으면 크게 조정할 필요가 있음
model = SoftmaxClassifierModel()
optimizer = optim.SGD(model.parameters(), lr=1e-1)

train(model, optimizer, x_train, y_train)
### Data Preprocessing
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [74, 66, 70]])

y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
# Standardization -> 정규 분포화
mu = x_train.mean(dim=0)
sigma = x_train.std(dim=0)
norm_x_train = (x_train - mu) / sigma
print(norm_x_train)
class MultivariateLinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3, 1)
    def forward(self, x):
        return self.linear(x)
    
model = MultivariateLinearRegressionModel()

optimizer = optim.SGD(model.parameters(), lr=1e-1)
def train(model, optimizer, x_train, y_train):
    nb_epochs = 20
    for epoch in range(nb_epochs):
        pred = model(x_train)
        cost = F.mse_loss(pred, y_train)
        
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        
        print(f"Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f}")
        

train(model, optimizer, x_train, y_train)
model = MultivariateLinearRegressionModel()

optimizer = optim.SGD(model.parameters(), lr=1e-1)

train(model, optimizer, norm_x_train, y_train)
전처리를 통해 모든 데이터를 보게 만듬
## Lab-07-2 MNIST Introduction
- MNIST
- torchvision
- Epoch
- Batch size
- Iteration
### torchvision
- 데이터셋과 모델 아키텍쳐, 전처리 등을 제공하는 패키지
import torchvision.datasets as dsets
from torchvision import transforms

...
mnist_train = dsets.MNIST(root="./", train=True, transform=transforms.ToTensor(), download=True)
mnist_test = dsets.MNIST(root="./", train=False, transform=transforms.ToTensor(), download=True)
...
data_loader = torch.utils.DataLoader(Dataloader=mnist_train, batch_size=64, shuffle=True, drop_lats=True)
...
### torch.no_grad()
- 학습하지 않겠다라는 의미
# Test
with torch.no_grad():
    X_test = mnist_test.test_data.view(-1, 28*28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)
    
    prd = linear(X_test)
    correct_pred = torch.argmax(pred, 1)==Y_test
    acc = correct_pred.float().mean()
    print(f"ACC: {acc.item()}")
